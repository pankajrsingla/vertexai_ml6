{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88fdc3cf-e013-4bef-b02d-9bf630bf4344",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "\n",
    "This notebook demonstrates how to use the `Vertex AI SDK` to train and deploy a custom model with `Scikit-learn pipeline and LightGBM` classifier for serving online predictions with explanations. Although Vertex AI has provided quite a few examples for deploying models built using *Tensorflow/SKLearn/XGBoost*, there are very few working examples explaining the deployment of **custom container** models, and almost none that show how to get **explainable predictions** for such models. This notebook is meant to bridge that gap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bd74d9-c141-43a7-abdf-7dcaff7b6d95",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset used for this tutorial is the [`Titanic dataset`](https://www.kaggle.com/competitions/titanic/data). Given certain categorical and numerical predictors, the model predicts the survival status of the passengers on the titanic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d3b40b-66be-4bb6-a588-f57f59f9bf69",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "This notebooks explains the process of training and deploying a custom classifier model to serve explainable predictions using Vertex AI SDK.<br>\n",
    "As an alternative, you can also use the gcloud command-line tool or the Vertex AI Cloud Console.\n",
    "\n",
    "The notebook follows the following broad steps:\n",
    "\n",
    "- Training a `Scikit-learn Pipeline + LightGBM` classifier model using a Vertex AI custom training job\n",
    "\n",
    "- Setting up a custom Docker container for serving online predictions\n",
    "\n",
    "- Configuring the model to provide explainable predictions\n",
    "\n",
    "- Uploading and deploying the trained model to Vertex AI\n",
    "\n",
    "- Generating explainable online predictions from the deployed model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64729cfe-8cd9-42fb-a9f4-ca1cb4e7d15c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setting up the development environment\n",
    "\n",
    "If you are using either Vertex AI Workbench Notebook or Google Colab, you can skip this step, since the environment already satisfies the requirements to run this notebook. <br> If not, make sure that your environment meets the following requirements:\n",
    "\n",
    "- The Cloud Storage SDK\n",
    "- Python 3\n",
    "- virtualenv\n",
    "- Jupyter notebook (running in a virtual environment with Python 3)\n",
    "\n",
    "#### Instructions on how to meet these requirements:\n",
    "\n",
    "1. [Cloud Storage SDK](https://cloud.google.com/sdk/docs/)\n",
    "\n",
    "2. [Python 3](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "3. [Virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
    "\n",
    "4. Activate your virtual environment and run `pip3 install Jupyter` in a terminal shell to install Jupyter.\n",
    "\n",
    "5. Run `jupyter notebook` on the command line in a terminal shell to launch Jupyter.\n",
    "\n",
    "6. Open this notebook in the Jupyter Notebook Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603aefdc-9061-462a-881c-d57235697b97",
   "metadata": {},
   "source": [
    "### Installing packages\n",
    "\n",
    "Install the Python packages required to execute this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dec5782-d98c-4a82-9e76-49e080feb461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\") and not os.getenv(\"VIRTUAL_ENV\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "! pip3 install --upgrade google-cloud-aiplatform $USER_FLAG -q\n",
    "! pip3 install {USER_FLAG} --upgrade google-cloud-storage -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b65333d-67d9-4718-83ea-f40fa2195239",
   "metadata": {},
   "source": [
    "### Set up your Google Cloud project\n",
    "\n",
    "*The following steps are required regardless of your notebook environment.*\n",
    "\n",
    "1. [Select/create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager).\n",
    "\n",
    "2. [Enable billing for your project.](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
    "\n",
    "3. [Enable the following APIs: Vertex AI APIs, Compute Engine APIs, and Cloud Storage.](https://console.cloud.google.com/flows/enableapi?apiid=ml.googleapis.com,compute_component,storage-component.googleapis.com)\n",
    "\n",
    "4. [The Google Cloud SDK](https://cloud.google.com/sdk) is already installed in Google Cloud Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7100f00-ecc2-4b78-9b24-b31da812ee8c",
   "metadata": {},
   "source": [
    "### Project variables\n",
    "#### For this notebook, you will need:\n",
    "- [Project ID](https://cloud.google.com/vertex-ai/docs/pipelines/configure-project)\n",
    "<br> The ID of your **billable** Google Cloud project.\n",
    "- [Cloud Storage Bucket](https://cloud.google.com/storage/docs/creating-buckets)\n",
    "<br> Containers for storing objects on Google Cloud.\n",
    "- [Region](https://cloud.google.com/vertex-ai/docs/general/locations)\n",
    "<br> The region where you want to deploy your model and store the model artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2754d387-ea2c-4443-9b3a-e020adbd1238",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"my_project_id\" # Has to be the project ID of your billable GCP\n",
    "MODEL_NAME = \"my_model\" # Any memorable string\n",
    "VERSION = \"v1\" # # Any memorable string/number\n",
    "BUCKET_NAME = \"my_bucket\" # Has to be created first\n",
    "REGION = \"europe-west1\" # Choose as per your location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef39469e-2dbd-45c7-84a3-4fd015915e93",
   "metadata": {},
   "source": [
    "### Additional variables\n",
    "- [Image URI](https://cloud.google.com/container-registry/docs/pushing-and-pulling)\n",
    "<br> The Artifact Registry or Container Registry URI of your container images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78e35c6-59b8-4b19-9ece-33e4d7091e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVING_MACHINE_TYPE = \"n1-standard-2\" # Update based on your requirements\n",
    "SERVING_GPU, SERVING_NGPU = (None, None) # example: (aip.gapic.AcceleratorType.NVIDIA_TESLA_K80.name, 2)\n",
    "ARTIFACT_LOCATION_GCS = f\"gs://{BUCKET_NAME}\"\n",
    "TRAIN_IMAGE_URI = f\"eu.gcr.io/{PROJECT_ID}/{MODEL_NAME}:{VERSION}\" # Differs based on region\n",
    "PRED_IMAGE_URI = f\"eu.gcr.io/{PROJECT_ID}/{MODEL_NAME}-pred:{VERSION}\" # Differs based on region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8a6c96-7874-4de4-9d4d-780544f181b8",
   "metadata": {},
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "If you are using\n",
    "\n",
    "A) `Workbench AI Notebooks`\n",
    "<br> Environment is already authenticated. Skip this step.\n",
    "\n",
    "B) `Colab`\n",
    "<br> Authenticate your account via oAuth using the cell below.\n",
    "\n",
    "C) `Other Environments`\n",
    "<br> Follow [authentication for Google Cloud Account.](https://cloud.google.com/docs/authentication/getting-started)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef95004f-4ec7-4428-a896-150435d8a303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account.\n",
    "\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
    "    \"DL_ANACONDA_HOME\"\n",
    "):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        print(\"Not Testing\")\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS \"<path_to_credentials.json_file>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959d0132-b7e1-466b-9983-0b3e17d626b0",
   "metadata": {},
   "source": [
    "### Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff73ea4c-72a4-40cf-935b-a6ab651a0f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "aip.init(project=PROJECT_ID, staging_bucket=ARTIFACT_LOCATION_GCS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167beccc-a4c9-4f3d-bfaa-a71774c62514",
   "metadata": {},
   "source": [
    "### Create model directory structure and docker files\n",
    "Depending on whether you use FastAPI/Flask/another web framework to set up the server for predictions, the directory structure can be slightly different.<br>\n",
    "We will use `FastAPI` in this notebook.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7e2a28-a7f7-41b6-a5b8-e24f96e96d14",
   "metadata": {},
   "source": [
    "### Directory structure for FastAPI server\n",
    "\n",
    "📁 model<br>\n",
    "└──📁 training<br>\n",
    "└────📋 Dockerfile<br>\n",
    "└────📋 requirements.txt<br>\n",
    "└────📁 trainer<br>\n",
    "└──────📋 train.py<br>\n",
    "\n",
    "└──📁 inference<br>\n",
    "└────📋 Dockerfile<br>\n",
    "└────📋 requirements.txt<br>\n",
    "└────📁 app<br>\n",
    "└──────📋 main.py<br>\n",
    "└──────📋 server.py<br>\n",
    "└──────📋 prestart.sh<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a5d53f-61aa-4501-bfb3-1531e8ccc481",
   "metadata": {},
   "source": [
    "### Download the code for the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaf2df6-f1fa-47c9-9541-24ff5cfb3080",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/pankajrsingla/vertex_ai.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8469b1-27d2-441d-a1cf-f4097b9040a6",
   "metadata": {},
   "source": [
    "### 1. Building and training the model\n",
    "We train a `Scikit-learn pipeline + LightGBM classifier` model on the Titanic dataset.<br>\n",
    "The trained model predicts the survival status of the passengers. <br>\n",
    "\n",
    "*For details about hyperparameter tuning and explainability, take a look at:*<br>\n",
    "*https://github.com/ml6team/quick-tips/blob/main/structured_data/2021_02_26_scikit_learn_pipelines/scikit_learn_pipelines_and_lightgbm_titanic_dataset.ipynb*\n",
    "\n",
    "We follow the following steps:<br>\n",
    "- Data loading and inspection\n",
    "- Introduction of the preprocessing pipeline\n",
    "- Adding a LightGBM classifier to the pipeline\n",
    "- Training the model\n",
    "- Saving the model artifact file (pickle) to GCP bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3fe740-4ae7-4686-b10e-944504ec34ec",
   "metadata": {},
   "source": [
    "#### 1.1 The training code\n",
    "Update the bucket name in the training file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6981bebd-3d1c-405b-b7e3-7896ef83e3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"vertex_ai/skl-lgbm/training\")\n",
    "f = open(\"trainer/train.py\",'a')\n",
    "f.write(\"bucket_name = \\\"\" + BUCKET_NAME + \"\\\"\")\n",
    "f.write(\"\"\"\n",
    "model_name = \"skl_lgbm.pkl\"\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "blob = bucket.blob(model_name)\n",
    "with blob.open(\"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "    \n",
    "print(\"Done training.\")\"\"\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb8010e-cd13-4b96-aa63-553488bfa07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat trainer/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f10647-5326-43ed-abf4-a52da6fd680c",
   "metadata": {},
   "source": [
    "#### 1.2 Module dependencies for training container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc4f17e-ec16-4ab0-886b-f179b2806d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd80ad8e-462c-4760-833f-1bbf7d95ecea",
   "metadata": {},
   "source": [
    "#### 1.3 Dockerfile for training container\n",
    "For a custom training image, we need to create a Dockerfile with the required details.<br>\n",
    "We specify the python image, copy the training code, install the dependencies, and set the entrypoint for the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1da0f5-0d65-477e-8942-f9603c5fdc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0b3d50-68a8-4efe-8a0e-77026d6a300b",
   "metadata": {},
   "source": [
    "#### 1.4 Build the training image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb482eb-6ee3-44d8-9a74-e96d97caddf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker build ./ -t $TRAIN_IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0cad48-2eea-4957-ab17-cc6750e45495",
   "metadata": {},
   "source": [
    "#### 1.5 (Optional) Run the training image\n",
    "This step can be skipped if you want the training to be done on Vertex AI servers, for which a custom training job has been defined in the next section.<br>\n",
    "Running the training image initiates the training locally and uploads the model artifact file to the GCP bucket once training is finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7747f609-f510-4f05-918f-7d563f0a4b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! docker run $TRAIN_IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bec80a3-a131-48a6-83c4-972a316084dc",
   "metadata": {},
   "source": [
    "#### 1.6 Push the training image to GCP Artifact Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49e01f8-0778-4974-8fa0-5baa157fc64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker push $TRAIN_IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a83eac-9784-4a03-8928-d6a9b2f11f6a",
   "metadata": {},
   "source": [
    "#### 1.7 Custom job for training the model on Vertex AI\n",
    "##### 1.7.1 Create a custom training job\n",
    "A custom training job is created with the following parameters:\n",
    "\n",
    "- `project`: The project ID\n",
    "- `display_name`: The human readable name for the custom training job.\n",
    "- `container_image_uri`: The training container image.\n",
    "- `location`: The region\n",
    "- `api_endpoint`: The endpoint for the specified region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9fca95-1dc8-43f3-9079-848f7f015524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_job_sample(\n",
    "    project: str,\n",
    "    display_name: str,\n",
    "    container_image_uri: str,\n",
    "    location: str = \"europe-west1\",\n",
    "    api_endpoint: str = \"europe-west1-aiplatform.googleapis.com\",\n",
    "):\n",
    "    # The AI Platform services require regional API endpoints.\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    # This client only needs to be created once, and can be reused for multiple requests.\n",
    "    client = aip.gapic.JobServiceClient(client_options=client_options)\n",
    "    custom_job = {\n",
    "        \"display_name\": display_name,\n",
    "        \"job_spec\": {\n",
    "            \"worker_pool_specs\": [\n",
    "                {\n",
    "                    \"machine_spec\": {\n",
    "                        \"machine_type\": \"n1-standard-4\",\n",
    "                        \"accelerator_type\": aip.gapic.AcceleratorType.ACCELERATOR_TYPE_UNSPECIFIED,\n",
    "                        \"accelerator_count\": None,\n",
    "                    },\n",
    "                    \"replica_count\": 1,\n",
    "                    \"container_spec\": {\n",
    "                        \"image_uri\": container_image_uri,\n",
    "                        \"command\": [],\n",
    "                        \"args\": [],\n",
    "                    },\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    }\n",
    "    parent = f\"projects/{project}/locations/{location}\"\n",
    "    response = client.create_custom_job(parent=parent, custom_job=custom_job)\n",
    "    print(\"response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804d2ea6-8d67-40fe-ae53-8c2da46af625",
   "metadata": {},
   "source": [
    "##### 1.7.2 Run the custom training job\n",
    "Depending upon the model and the size of the dataset, this can take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a74bfd-a30a-4113-b265-62397984bfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_custom_job_sample(project=PROJECT_ID, \n",
    "                         display_name=\"my_custom_job\", \n",
    "                         container_image_uri=TRAIN_IMAGE_URI,\n",
    "                         location=REGION,\n",
    "                         api_endpoint=f\"{REGION}-aiplatform.googleapis.com\"\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d83c6b8-0383-4141-a2a5-8f0fb8948453",
   "metadata": {},
   "source": [
    "### 2. FastAPI App server for serving predictions\n",
    "We need:<br>\n",
    "- `server.py`: Creates the model server.<br>\n",
    "- `main.py`: Creates the HTTP server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd93fe7d-9b1d-4291-b12e-4386eb0797ec",
   "metadata": {},
   "source": [
    "#### 2.1 Create the model server\n",
    "This loads the model artifact file from GCP and generates the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b6e000-ec65-4535-b9e9-d1eff956d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../inference\")\n",
    "! cat app/server.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cc4e05-415a-4a4a-98f4-967f35d15093",
   "metadata": {},
   "source": [
    "#### 2.2 Create the (FastAPI) HTTP server\n",
    "We will need an HTTP server in the deployment container to handle the `predict` and `health` requests. This server is akin to an additional layer on top of the model server.<br>\n",
    "We build the HTTP server using FastAPI.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97db845-31a2-426b-940c-f3213a6aca20",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat app/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77a8b9a-90c9-4136-9a40-1877d308c3ca",
   "metadata": {},
   "source": [
    "#### 2.3 Add the pre-start script\n",
    "\n",
    "FastAPI will execute this script before starting up the server. The `PORT` environment variable is set to equal `AIP_HTTP_PORT` in order to run FastAPI on the same port expected by Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ade8141-7c6e-4622-899a-5a87b1f10fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat app/prestart.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acf9f8f-f81e-4c7b-8df6-77364d0ed617",
   "metadata": {},
   "source": [
    "### 3. Prediction Container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b5c97b-c9d2-468f-97f7-c41cdafca0bf",
   "metadata": {},
   "source": [
    "#### 3.1 Module dependencies for prediction container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2264d1ae-64b4-4e0f-bd2f-c74457b7e1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aade7f-c7ee-455a-80ce-269571be9e6a",
   "metadata": {},
   "source": [
    "#### 3.2 Dockerfile for prediction container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4742c0d1-df29-49a1-9992-8f6dc708b83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbb9a14-4975-4515-abce-0e93fbc03509",
   "metadata": {},
   "source": [
    "#### 3.3 Build the prediction image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4964b07a-c31d-4209-8d9f-9d2c11bffacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker build ./ -t $PRED_IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2d1485-a2cb-43c4-9533-559157b1d85b",
   "metadata": {},
   "source": [
    "#### 3.4 (Optional) Test the prediction server locally\n",
    "Run the container locally in detached mode and provide the environment variables that the container requires. These variables will be provided to the container by Vertex prediction once deployed. Test the /health and /predict routes, then stop the running image.<br>\n",
    "*This step can save a lot of time, as it allows you to correct any errors in the docker/model configuration without having to wait for the model to be uploaded and deployed.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243bcabf-b94f-4d70-913c-167c9865c51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker run -p \"80:8080\" --name=\"local-skl-lgbm\" -e \"AIP_HTTP_PORT=8080\" -e \"AIP_HEALTH_ROUTE=/health\" -e \"AIP_PREDICT_ROUTE=/predict\" -e \"AIP_STORAGE_URI=$ARTIFACT_LOCATION_GCS\" --rm  $PRED_IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0139fe9c-8483-4a08-8f27-d4bc758cd18e",
   "metadata": {},
   "source": [
    "##### 3.4.1 Test the health route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cb6ad2-a752-4bd0-b56b-97ccd9d251e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl localhost/health"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffde311-97c9-4a28-a793-368448c39894",
   "metadata": {},
   "source": [
    "##### 3.4.2 Test the predict route\n",
    "Create a json file with some test inputs and check the model predictions for these inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3829fd-fcf4-4b7a-b725-9bbf3b92dfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile instances.json\n",
    "{\n",
    "    \"instances\": [{\n",
    "        \"Pclass\": 3, \n",
    "        \"Sex\": \"female\", \n",
    "        \"Age\": 14.1, \n",
    "        \"Fare\": 11, \n",
    "        \"Embarked\":\"C\"\n",
    "    },{\n",
    "        \"Pclass\": 1, \n",
    "        \"Sex\": \"male\", \n",
    "        \"Age\": 11.1, \n",
    "        \"Fare\": 23, \n",
    "        \"Embarked\":\"S\"}]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7a990c-0719-4b46-92ba-3742fecc6db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -X POST \\\n",
    "  -d @instances.json \\\n",
    "  -H \"Content-Type: application/json; charset=utf-8\" \\\n",
    "  localhost/predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ea098a-37a9-4c9d-bfb0-6dc5c0d6618f",
   "metadata": {},
   "source": [
    "##### 3.4.3 Stop the local execution of the prediction image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf3701e-5c09-4d28-8cb0-dde6ae34c349",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker stop local-skl-lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8050dec-6fe1-45ea-888d-43fe462214a9",
   "metadata": {},
   "source": [
    "#### 3.5 Push the prediction image to GCP Artifact Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a7ca2f-ccf8-424f-9683-6da63ec44abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker push $PRED_IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb27e229-305f-4898-b30d-4049a14e3781",
   "metadata": {},
   "source": [
    "### 4. Set the explanation parameters and metadata\n",
    "These are required only for explainable predictions. You can avoid configuring these if you only want plain predictions from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e92757d-ee5e-4af6-8d25-ef61c6065921",
   "metadata": {},
   "source": [
    "#### 4.1 Explanation metadata\n",
    "The explanation metadata consists of:<br>\n",
    "- `outputs`: A scalar value in the output to attribute - what to explain.\n",
    "- `inputs`: The features for attribution - how they contributed to the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275bf558-4b6e-490c-b531-4fb3fa801c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_metadata = aip.explain.ExplanationMetadata(\n",
    "    inputs={\n",
    "        \"Pclass\": {},\n",
    "        \"Sex\": {},\n",
    "        \"Age\": {},\n",
    "        \"Fare\": {},\n",
    "        \"Embarked\": {}\n",
    "    },\n",
    "    outputs={\n",
    "        \"Survived\": {}\n",
    "    }\n",
    ")\n",
    "print(explanation_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47420b93-84b5-4f31-a03a-91570983dfee",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4.2 Explanation parameters\n",
    "You can choose between Shapley/Integrated Gradients/XRAI algorithms for explanability.<br>\n",
    "*For details on the three methods, refer to:*<br>\n",
    "*https://cloud.google.com/ai-platform/prediction/docs/ai-explanations/overview*\n",
    "\n",
    "We will use `Shapley` algorithm in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac7296a-d866-4949-94f6-3588914a7e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "XAI = \"shapley\"  # [ shapley, ig, xrai ]\n",
    "# Takes different permutations of the features, and assigns attribution for the outcome to each feature.\n",
    "# Path count can be set to a lower/higher value.\n",
    "\n",
    "if XAI == \"shapley\":\n",
    "    PARAMETERS = {\"sampled_shapley_attribution\": {\"path_count\": 10}}\n",
    "elif XAI == \"ig\":\n",
    "    PARAMETERS = {\"integrated_gradients_attribution\": {\"step_count\": 50}}\n",
    "elif XAI == \"xrai\":\n",
    "    PARAMETERS = {\"xrai_attribution\": {\"step_count\": 50}}\n",
    "\n",
    "explanation_parameters = aip.explain.ExplanationParameters(PARAMETERS)\n",
    "print(explanation_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110275ca-7676-4344-b66e-fd195001c5fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5. Upload the model\n",
    "\n",
    "Upload your model to a `Model` resource using the `Model.upload()` method, with the following parameters:\n",
    "\n",
    "- `display_name`: The human readable name for the `Model` resource\n",
    "- `artifact_uri`: The Cloud Storage location of the trained model artifacts\n",
    "- `serving_container_image_uri`: The serving container image\n",
    "- `explanation_parameters`: Parameters to configure explaining for `Model`'s predictions\n",
    "- `explanation_metadata`: Metadata describing the `Model`'s input and output for explanation\n",
    "- `serving_container_predict_route`: The route for sending prediction requests to the server\n",
    "- `serving_container_health_route`: The route for sending health check requests to the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de451826-ae47-4585-9e6d-265cb9cec8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, location=REGION)\n",
    "model = aip.Model.upload(\n",
    "    display_name=MODEL_NAME,\n",
    "    artifact_uri=ARTIFACT_LOCATION_GCS,\n",
    "    serving_container_image_uri=PRED_IMAGE_URI,\n",
    "    explanation_parameters=explanation_parameters,\n",
    "    explanation_metadata=explanation_metadata,\n",
    "    serving_container_predict_route=\"/predict\",\n",
    "    serving_container_health_route=\"/health\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ac7812-745d-4131-95ff-c9d65a4381de",
   "metadata": {},
   "source": [
    "### 6. Deploy the model\n",
    "\n",
    "Deploy your model for online prediction. To deploy the model, you invoke the `deploy` method, with the following main parameters:\n",
    "\n",
    "- `deployed_model_display_name`: A human readable name for the deployed model.\n",
    "- `explanation_metadata`: The metadata object for explanations\n",
    "- `explanation_parameters`: The algorithm for explanation and the corresponding paramaters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993f6d4b-6adb-40a9-86c2-dde5f62c41fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = model.deploy(\n",
    "    deployed_model_display_name=MODEL_NAME,\n",
    "    explanation_metadata=explanation_metadata,\n",
    "    explanation_parameters=explanation_parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4d4fd6-f5cd-4081-a278-b25ba69689fe",
   "metadata": {},
   "source": [
    "### 7. Make predictions\n",
    "\n",
    "Once the `Model` resource is deployed to an `Endpoint` resource, one can do online predictions/explanations by sending `predict` / `explain` requests to the `Endpoint` resource.\n",
    "\n",
    "#### Request\n",
    "\n",
    "The format of each instance is:\n",
    "\n",
    "    [feature_list]\n",
    "\n",
    "Since the `predict()` / `explain()` methods can take multiple items (instances), if you have a single test item, send it as a list of one item.\n",
    "\n",
    "#### Response\n",
    "\n",
    "The response from the `predict()` / `explain()` call is a Python dictionary with the following entries:\n",
    "\n",
    "- `ids`: The internal assigned unique identifiers for each prediction request.\n",
    "- `predictions`: The prediction per instance.\n",
    "- `deployed_model_id`: The Vertex AI identifier for the deployed `Model` resource which did the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c27c365-4e10-4412-b260-678e58df01f6",
   "metadata": {},
   "source": [
    "#### 7.1 Predictions without explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cfc6a6-201c-4521-a607-7606b773d736",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = [{\"Pclass\": 3, \"Sex\": \"female\", \"Age\": 14.1, \"Fare\": 11, \"Embarked\":\"C\"}, \n",
    "             {\"Pclass\": 3, \"Sex\": \"male\", \"Age\": 11.1, \"Fare\": 23, \"Embarked\":\"C\"}]\n",
    "predictions_plain = endpoint.predict(instances=instances)\n",
    "print(\"Plain predictions:\", predictions_plain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f9319c-6329-49ec-9f03-2b9fcb007d79",
   "metadata": {},
   "source": [
    "#### 7.2 Predictions with explanations\n",
    "The input and response format for explainable predictions is the same as for predictions, except that instead of a `predict` call, we make an `explain` call to the `Endpoint` resource.\n",
    "\n",
    "The response from `explain()` call is a Python dictionary similar to what we get from a `predict()` call, with an additional `explanations` entry:\n",
    "- `explanations` (optional): The feature attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb6115e-d371-48d4-9fed-c9fd3b0ce505",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations = endpoint.explain(instances)\n",
    "print(\"Explainable predictions:\", explanations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64608bf-6ecd-4b87-9d8a-b23d20b91338",
   "metadata": {},
   "source": [
    "#### 7.3 Examining the explanation attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c68e5dd-449d-4e88-b239-c2550015d7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTANCE = {\"Pclass\": 3, \"Sex\": \"female\", \"Age\": 14.1, \"Fare\": 11, \"Embarked\":\"C\"}\n",
    "from tabulate import tabulate\n",
    "\n",
    "feature_names = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\"]\n",
    "explanation = endpoint.explain(instances=[INSTANCE])\n",
    "attributions = explanation.explanations[0].attributions[0].feature_attributions\n",
    "\n",
    "rows = []\n",
    "for i, val in enumerate(feature_names):\n",
    "    rows.append([val, INSTANCE[val], attributions[val]])\n",
    "print(tabulate(rows, headers=[\"Feature name\", \"Feature value\", \"Attribution value\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df9a175-810f-4055-b113-b10e843ebd47",
   "metadata": {},
   "source": [
    "### 8. Cleaning up\n",
    "When you are done getting predictions/explanations, undeploy the model from the `Endpoint` resouce. This deprovisions all compute resources and ends billing for the deployed model. Additionally, you can also delete the uploaded model, and delete the artifacts from the GCP bucket, as well as delete the training and prediction images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35db6dbc-2ccf-4dc4-bdab-8d9f16b7ded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Delete endpoint\n",
    "    endpoint.delete(force=True)\n",
    "    # Delete the model resource\n",
    "    model.delete()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Delete model artifacts from bucket\n",
    "! gsutil -m rm -rf gs://$BUCKET_NAME/skl_lgbm.pkl\n",
    "\n",
    "# Delete training and prediction images:\n",
    "# ! docker images\n",
    "# ! docker rmi <training_image_id> <prediction_image_id> -f"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu101.m84",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu101:m84"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
